{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style = 'text-align: center'><b> Text Preprocessing With NLTK</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>1. Accessing Text From The Web And Disk</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>a. Electronic Books</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division  # Python 2 users only\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sử dụng url để lấy text \n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "# đọc và mã hóa dữ liệu text thành utf8 \n",
    "raw = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print một số thông tin của text\n",
    "\n",
    "# kiểu dữ liệu được lấy về dưới dạng string\n",
    "print(type(raw))\n",
    "\n",
    "# độ dài dữ liệu\n",
    "print(len(raw))\n",
    "\n",
    "# dữ liệu thô tồn tại một số ký tự không mong muốn như \\r hay \\n,...\n",
    "raw[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thực hiện phân tách thành các từ đơn - TOKENIZATION\n",
    "# nltk.data.path.append('.venv/nltk_data')\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "# check kiểu dữ liệu của tokens - Python list\n",
    "print(type(tokens))\n",
    "\n",
    "# số lượng tokens \n",
    "print(len(tokens))\n",
    "\n",
    "# in 10 tokens đầu\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chuyển đổi kiểu dữ liệu đặc trưng của nltk - nltk.text()\n",
    "tokens_text = nltk.Text(tokens)\n",
    "\n",
    "# check kiểu dữ liệu của tokens_text\n",
    "print(type(tokens_text))\n",
    "\n",
    "# slicing: cắt một đoạn của tokens_text từ vị trí 1024 - 1062\n",
    "print(tokens_text[1024:1062])\n",
    "\n",
    "# cho biết các cặp từ xuất hiện cùng nhau thường xuyên\n",
    "print(tokens_text.collocations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tìm kiếm và tinh chỉnh nội dung (lọc thủ công) giúp cho loại bỏ các thông tin nhiễu và chỉ giữ lại nội dung chính\n",
    "# tìm từ trên xuống\n",
    "print(raw.find(\"PART I\"))\n",
    "# tìm từ dưới lên - rfind = reverse find\n",
    "print(raw.rfind(\"*** END OF THE PROJECT GUTENBERG EBOOK CRIME AND PUNISHMENT ***\"))\n",
    "# nội dung chính sẽ bắt đầu từ 5575 - 1158053\n",
    "raw_text = raw[5575:1158049]\n",
    "\n",
    "raw_text[-100:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>b. Dealing with HTML</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lấy dữ liệu từ html qua url \n",
    "url = \"https://www.medicalnewstoday.com/articles/could-some-diets-help-manage-long-covid\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "print(html[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lấy văn bản ra khỏi HTML bằng thư viện Python: BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "# tách thành các từ đơn - tokenize\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sử dụng concordance để tìm kiếm và hiển thị một từ trong khoảng 110->390\n",
    "sub_tokens = tokens[110: 390]\n",
    "text_tokens = nltk.Text(sub_tokens)\n",
    "text_tokens.concordance('COVID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>c. Processing RSS Feeds</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "# lấy thông tin từ url \n",
    "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llog)\n",
    "print(llog.keys())\n",
    "\n",
    "# llog['feed']: Truy cập thông tin chung của nguồn cấp dữ liệu (RSS feed)\n",
    "print(len(llog['feed']))\n",
    "\n",
    "# llog.entries: lấy danh sách các danh mục có trong RSS\n",
    "print(len(llog.entries))\n",
    "print(llog['feed']['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lấy một danh mục từ dữ liệu \n",
    "post = llog.entries[2]\n",
    "\n",
    "# lấy nội dung trong danh mục đó: kq sẽ trả ra nội dung dưới dạng HTML (các thẻ với nội dung bên tring)\n",
    "content = post.content[0].value\n",
    "\n",
    "print(f\"Entry title: {post.title}\\n\")\n",
    "print(f'Entry content: \\n{content}\\n')\n",
    "print(f'Sub entry content: \\n{content[:70]}')\n",
    "print()\n",
    "\n",
    "# lấy text ra khỏi html \n",
    "raw_data = BeautifulSoup(content, 'html.parser').get_text()\n",
    "print(\"Raw data: \\n\", raw_data[:100])\n",
    "print()\n",
    "\n",
    "# thực hiện tokenize\n",
    "tokens = word_tokenize(raw_data)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>d. Local Files</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# đọc file cục bộ\n",
    "import os\n",
    "\n",
    "# listdir thực hiện liệt kê tất cả các file có trong thư mục \n",
    "os.listdir('../Documents')      # kiểm tra các file có trong folder Documents\n",
    "\n",
    "# mở và dọc nội dung file\n",
    "f = open(\"../Documents/document.txt\", 'r')\n",
    "raw_data = f.read()     # text sẽ được lưu dưới dạng string\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hoặc có thể đọc dữ liệu theo từng dòng: \n",
    "f = open(\"../Documents/document.txt\", 'rU')\n",
    "for line in f:\n",
    "    print(line.strip())     # loại bỏ các ký tự thừa (khoảng trắng ở đầu và cuối dòng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus: là các tập dữ liệu ngôn ngữ học được cung cấp bởi thư viện Natural Language Toolkit (NLTK). \n",
    "# NLTK cung cấp nhiều loại corpus (tập dữ liệu ngôn ngữ) để phục vụ cho các mục đích phân tích và xử lý ngôn ngữ tự nhiên.\n",
    "\n",
    "# sử dụng .find() để tìm tài liệu tương ứng\n",
    "path = nltk.data.find('corpora/gutenberg/melville-moby_dick.txt')\n",
    "# thực hiện mở và đọc file như bình thường\n",
    "raw = open(path, 'rU').read()\n",
    "\n",
    "raw[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sử dụng input để lấy thông tin nhập từ bàn phím của user\n",
    "input_text = input(\"Enter some text: \")\n",
    "print(input_text)\n",
    "# tokenize \n",
    "tokens = word_tokenize(input_text)\n",
    "print(f\"You typed {len(tokens)} words: \")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>e. NLP Pipeline</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = open(\"../Documents/document.txt\").read()\n",
    "\n",
    "tokens = word_tokenize(raw_data)\n",
    "\n",
    "words = [w.lower() for w in tokens]\n",
    "\n",
    "vocab = sorted(set(words))\n",
    "vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>2. String: Text Processing At The Lowest Level</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 6, 5, 4, 3, 2, 1]\n",
    "b = [' ' * 2 * (7 - i) + 'very' * i for i in a]\n",
    "for line in b:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>3. Text Processing With Unicode</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file được sử dụng là dạng mã hóa Latin2\n",
    "path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')\n",
    "\n",
    "# thêm parameter encoding để đánh dấu\n",
    "f= open(path, encoding='latin2')\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord('ń')        # cho ra số thứ tự nguyên của từ: 324 \n",
    "\n",
    "nacute = '\\u0144'       # 324 (dec) -> 0144 (hex)\n",
    "nacute\n",
    "\n",
    "nacute.encode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read vietname text in file (check với văn bản tiếng việt)\n",
    "path = '../Documents/Vietnamese.txt'\n",
    "f = open(path, encoding='utf-8')\n",
    "raw_data = f.read()\n",
    "# \n",
    "print(raw_data)\n",
    "raw_data\n",
    "\n",
    "# tokenization\n",
    "tokens = word_tokenize(raw_data)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# đánh dấu cho trình biên dịch biết rằng sẻ sử dụng bảng mã utf-8 trong phần này\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Chuỗi chứa ký tự đặc biệt (tiếng Việt có dấu)\n",
    "chuoi = \"Chào mừng bạn đến với Python! Đừng quên học hỏi mỗi ngày.\"\n",
    "\n",
    "# In ra chuỗi\n",
    "print(chuoi)\n",
    "\n",
    "# Ghi chuỗi vào tệp với mã hóa UTF-8\n",
    "with open('example.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(chuoi)\n",
    "\n",
    "# Đọc chuỗi từ tệp và in ra màn hình\n",
    "with open('example.txt', 'r', encoding='utf-8') as f:\n",
    "    noi_dung = f.read()\n",
    "    print(noi_dung)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>4. Regular Expressions for Detecting Word Patterns</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>a. Basic meta-characters</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# sử dụng words để truy cập vào bộ từ vựng (corpus) tiếng Anh ('en') từ thư viện NLTK.\n",
    "# từ đó, trả về một danh sách tất cả các từ tiếng Anh có trong corpus này.\n",
    "wordlist = [w for w in nltk.corpus.words.words('en') \n",
    "            if w.islower()]        # điều kiện để giữ lại các chữ thường (loại bỏ các từ in hoa) hoặc tên riêng\n",
    "wordlist[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tìm kiếm ký tự bằng re.search(p,s): tìm kiếm chuỗi p trong s\n",
    "ed_list = [w for w in wordlist \n",
    "           if re.search('ed$', w)]      # sử dụng regular expression <<ed$>> để tìm kiếm các từ có đuôi ed trong wordlist\n",
    "                                        # dấu '$' thông báo cho kết thúc chuỗi\n",
    "ed_list\n",
    "\n",
    "# '.' đại diện cho một ký tự bất kì nào trong chuỗi\n",
    "searching_list = [w for w in wordlist \n",
    "                  if re.search('^..j..t..$', w)]    # '^' đại diện cho bắt đầu, '$' đại diện cho kết thúc\n",
    "searching_list      # kết quả trả ra các từ có j ở vị trí 3, t ở vị trí 6, và sau đó là 2 ký tự cuối\n",
    "\n",
    "\n",
    "searching_list = [w for w in wordlist \n",
    "                  if re.search('..j..t..', w)]  # nếu loại bỏ '^' và '$' thì việc tìm kiếm sẽ trở thành các từ có j và t ở giữa câu,\n",
    "                                                # và khoảng cách giữa j & t là 2 ký tự\n",
    "searching_list      # kết quả trả ra các từ có j ở vị trí 3, t ở vị trí 6, và sau đó là 2 ký tự cuối\n",
    "\n",
    "# đếm số lần xuất hiện của từ trong văn bản\n",
    "text = '''\n",
    "Email (hoặc e-mail, viết tắt của \"electronic mail\") là một phương tiện trao đổi thông tin điện tử qua mạng Internet. Email cho phép người dùng gửi tin nhắn văn bản, tài liệu, hình ảnh, và nhiều loại tệp đính kèm khác cho một hoặc nhiều người nhận. Đây là một trong những công cụ giao tiếp kỹ thuật số phổ biến và quan trọng nhất trên thế giới hiện nay.\n",
    "\n",
    "Email thường bao gồm ba phần chính:\n",
    "1. Địa chỉ người nhận: Một địa chỉ email có định dạng chung là tênngười@tênmiền.com. Ví dụ: example@gmail.com.\n",
    "2. Tiêu đề (Subject): Mô tả ngắn gọn nội dung của email, giúp người nhận biết được thông điệp chính trước khi mở thư.\n",
    "3. Nội dung (Body): Phần chính của email chứa thông điệp mà người gửi muốn truyền tải.\n",
    "\n",
    "Ngoài ra, email có thể bao gồm các tệp đính kèm như tài liệu, hình ảnh hoặc video. Phần mềm email cũng hỗ trợ các chức năng như trả lời tự động, chuyển tiếp thư (forward), và tổ chức thư mục (folders) để quản lý thư một cách hiệu quả.\n",
    "\n",
    "Email có nhiều ưu điểm, bao gồm khả năng gửi tin nhắn tức thời, tiết kiệm chi phí so với thư tín truyền thống, và khả năng lưu trữ thông tin dễ dàng. Tuy nhiên, cùng với sự phát triển của email, các vấn đề như thư rác (spam) và bảo mật thông tin cá nhân cũng trở nên phổ biến, đòi hỏi các biện pháp phòng chống an ninh mạng để bảo vệ người dùng.\n",
    "\n",
    "Ngày nay, email không chỉ là phương tiện liên lạc cá nhân mà còn là công cụ quan trọng trong công việc, kinh doanh, và tiếp thị trực tuyến. Dịch vụ email phổ biến bao gồm Gmail, Outlook, và Yahoo Mail.\n",
    "'''\n",
    "# tokenize\n",
    "text = text.lower()\n",
    "# tokens = word_tokenize(text)\n",
    "# count_occurence = sum(1 for w in tokens if re.search('^e-?mail$', w))\n",
    "# count_occurence\n",
    "\n",
    "word = '^e-?mail$'\n",
    "print(re.search('^e-?mail$', text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>b. Ranges and Closures</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# các ký tự theo thứ tự (như cách bấm bàn phím số)\n",
    "text_on_keys = [w for w in wordlist if re.search('^[ghi][mno][jkl][def]$', w)]\n",
    "text_on_keys\n",
    "\n",
    "text_on_keys = [w for w in wordlist if re.search('^[ghijklmno]+$', w)]\n",
    "text_on_keys\n",
    "\n",
    "text_on_keys = [w for w in wordlist if re.search('^[g-o]+$', w)]\n",
    "text_on_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\n",
    "text_list = [w for w in chat_words if re.search('^m+i+n+e+$', w)]\n",
    "text_list\n",
    "\n",
    "text_list = [w for w in chat_words if re.search('^[ha]+$', w)]\n",
    "text_list\n",
    "\n",
    "text_list = [w for w in chat_words if re.search('^m*i*n*e*$', w)]\n",
    "text_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "\n",
    "# \\ dùng để phân biệt (thoát) meta-characters, xem nó là một ký tự thường (ở đây là .)\n",
    "[w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)]       # '.' lúc này là một ký tự thường; không là meta-characters\n",
    "\n",
    "[w for w in wsj if re.search('^[A-Z]+\\$$', w)]          # '$' đầu tiên là một ký tự dô-la bình thường\n",
    "\n",
    "[w for w in wsj if re.search('^[0-9]{4}$', w)]      # {n} khớp với n lần, có thể xem độ dài là 4\n",
    "\n",
    "[w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)]     # {n,m} khớp với ÍT NHẤT n lần, NHIỀU NHẤT m lần\n",
    "\n",
    "[w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]       # tương tự như ví dụ trên\n",
    "\n",
    "[w for w in wsj if re.search('(ed|ing)$', w)]           # toán tử hoặc |: khớp với một trong hai lựa chọn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>5. Useful Applications Of Regular Expression</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> a. Extracting Word Pieces</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sử dụng re.findall(pattern, string) để tìm kiếm tất cả pattern có trong string\n",
    "word = 'supercalifragilisticexpialidocious'\n",
    "re.findall(r'[aeiou]{2,}', word)      # find-all thực hiện tìm kiếm các ký tự và KHÔNG LẶP LẠI với (ký tự ở vị trí trước đó)\n",
    "\n",
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "fd = nltk.FreqDist(vs for word in wsj           # tạo ra một chuỗi với tần suất xuất hiện \n",
    "                      for vs in re.findall(r'[aeiou]{2,}', word))       # tìm kiếm các ký tự có ít nhất 2 nguyên âm nằm liên tiếp nhau\n",
    "fd.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[int(n) for n in re.findall(r'[0-9]+', '2009-12-31')]      # \\d đại diện cho các ký tự là số (0-9), + đại diện cho các số viết liền nhau "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>b. Doing More with Word Pieces</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rút gọn từ (các nguyên âm trong từ)\n",
    "word = 'supercalifragilisticexpialidocious'\n",
    "\n",
    "# thứ tự của các regexp có thể ảnh hướng đến kết quả tìm kiếm (với các trường hợp bị chồng nhau): do nó duyệt điều kiện từ TRÁI -> PHẢI\n",
    "regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'   # biểu thức [^AEIOUaeiou]: lấy các kí tự không nằm trong [] (^ mang nghĩa phủ định)\n",
    "\n",
    "def compress(word):\n",
    "    pieces = re.findall(regexp, word)\n",
    "    return ''.join(pieces)\n",
    "\n",
    "compress('expectation')\n",
    "english_udhr = nltk.corpus.udhr.words('English-Latin1')\n",
    "\n",
    "print(\"Before compressing words:\\n\", nltk.tokenwrap(english_udhr[:75]))\n",
    "print()\n",
    "print(\"After compressing words:\\n\",nltk.tokenwrap(compress(w) for w in english_udhr[:75]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phân tích các cặp phụ âm-nguyên âm từ các từ trong ngôn ngữ Rotokas.\n",
    "rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')        # lấy từ điển Rotokas \n",
    "cvs = []\n",
    "cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]       # tìm kiếm các cặp phụ-nguyên âm\n",
    "cfd = nltk.ConditionalFreqDist(cvs)\n",
    "\n",
    "print(cfd.tabulate())\n",
    "\n",
    "# Sử dụng nltk.Index để lấy từ tương ứng với bảng thống kê\n",
    "cv_word_pairs = [(cv, w) for w in rotokas_words         # lưu cặp phụ âm và từ tương ứng\n",
    "                         for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
    "cv_index = nltk.Index(cv_word_pairs)        # sử dụng Index để tạo chỉ mục như (key,value): key - cv, value - w\n",
    "cv_index['su']      # gọi key để lấy value\n",
    "cv_index['te']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding word stems (Tìm từ gốc bằng thủ công)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Việc tìm từ gốc có nghĩa là loại bỏ hậu tố của nó và chỉ giữ lại các từ gốc ban đầu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tìm gốc từ - finding word stems \n",
    "\n",
    "# thực hiện loại bỏ các hậu tố\n",
    "\n",
    "# C1: sử dụng endswith\n",
    "suffix = ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']\n",
    "\n",
    "# trích từ gốc (bỏ hậu tố)\n",
    "def stem(word):\n",
    "    for suf in suffix:\n",
    "        if(word.endswith(suf)):\n",
    "            return word[:-len(suf)]\n",
    "    return word\n",
    "        \n",
    "# C2: sử dụng regular expression để trích hậu tố ra\n",
    "def stem_regexp(word):\n",
    "    return re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', \n",
    "                      word)      # các chuỗi trong () cho thấy rằng đó là những chuỗi con cần trích xuất\n",
    "\n",
    "stem_regexp('shieves')\n",
    "\n",
    "# tách chuỗi thành từ GỐC và HẬU TỐ\n",
    "def stem_suffix(word):\n",
    "    # ()(): tách thành 2 chuỗi\n",
    "    return re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$',  # (.*) trích xuất các từ trước hậu tố\n",
    "               word)\n",
    "\n",
    "\n",
    "def stem_suffix_improve(word):\n",
    "    # * là toán tử tham lam -> nó cố gắng lấy càng nhiều ký tự của đầu vào càng tốt\n",
    "    return re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$',      # sử dụng *? để cải thiện tình trạng này: khớp với ít ký tự nhất có thể trong khi vẫn đáp ứng điều kiện tổng thể của biểu thức chính quy.\n",
    "               word)\n",
    "\n",
    "stem_suffix_improve('language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Một chương trình tìm từ gốc hoàn thiện (tạm chấp nhận dù vẫn còn một số problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_word(word):\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'      # regular expression để trích xuất từ gốc và hậu tố\n",
    "    stem,suffix = re.findall(regexp,word)[0]           # do re.findall() trả về một danh sách, nên phải sử dụng [0] để lấy kq đầu tiên\n",
    "    return stem         # trả về từ gốc\n",
    "\n",
    "# văn bản thô \n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "... is no basis for a system of government.  Supreme executive power derives from\n",
    "... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "# tokenize\n",
    "tokens = word_tokenize(raw)\n",
    "stem_text = [stem_word(word) for word in tokens]\n",
    "\n",
    "print(nltk.tokenwrap(stem_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching Tokenized Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg, nps_chat\n",
    "moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))\n",
    "# sử dụng cú pháp .findall() cho phép kết hợp với <> để tìm kiếm với các tokens\n",
    "moby.findall(r\"<a> <.*> <man>\")       # tìm kiếm các từ nằm giữa <a> và <man> (được đánh dấu bằng () )\n",
    "\n",
    "chat = nltk.Text(nps_chat.words())\n",
    "chat.findall(r\"<.*> <.*> <bro>\")        # tìm kiếm các chuỗi có 2 cụm trước từ <bro>\n",
    "\n",
    "chat.findall(r\"<l.*>{3,}\")          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))\n",
    "\n",
    "hobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")      # \\w: đại diện cho các ký tự chữ, số và gạch dưới\n",
    "\n",
    "hobbies_learned.findall(r\"<as> <\\w*> <as> <\\w*>\")      # \\w: đại diện cho các ký tự chữ, số và gạch dưới\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>6. Normalizing Text</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trong các ví dụ chương trình trước đó, chúng ta thường chuyển đổi văn bản thành chữ thường trước khi thực hiện bất kỳ thao tác nào với các từ của nó, ví dụ: set(w.lower() for w in text). Bằng cách sử dụng lower(), chúng ta đã chuẩn hóa văn bản thành chữ thường để bỏ qua sự khác biệt giữa The và the.\n",
    "\n",
    "- Thường thì chúng ta muốn đi xa hơn thế này và loại bỏ mọi tiền tố, một nhiệm vụ được gọi là **stemming**.\n",
    "\n",
    "- Một bước nữa là đảm bảo rằng dạng kết quả là một từ đã biết trong từ điển, một nhiệm vụ được gọi là **lemmatization**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chuẩn bị dữ liệu\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "# tokenization\n",
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>a. Stemmers</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'lying', 'in', 'ponds', 'distributing', 'swords', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'masses', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n",
      "\n",
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n",
      "\n",
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "# Khởi tạo Stemmer Porter\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "# Khởi tạo Stemmer Lancaster\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "result_porter = [porter.stem(word) for word in tokens]\n",
    "\n",
    "result_lancaster = [lancaster.stem(word) for word in tokens]\n",
    "\n",
    "print(tokens)\n",
    "print()\n",
    "print(result_porter)\n",
    "print()\n",
    "print(result_lancaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 1824\n",
      "r king ! DENNIS : Listen , strange women lying in ponds distributing swords is no\n",
      "\n",
      "index: 6451\n",
      " beat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of\n",
      "\n",
      "index: 7038\n",
      "       Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded !   \n",
      "\n",
      "index: 7080\n",
      "doctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well  \n",
      "\n",
      "index: 8450\n",
      "ere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which \n",
      "\n",
      "index: 13860\n",
      "   you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --\n",
      "\n",
      "index: 13965\n",
      "h it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k\n",
      "\n",
      "index: 16684\n",
      "not stop our fight ' til each one of you lies dead , and the Holy Grail returns t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# khởi tạo lớp IndexedText để Lập chỉ mục văn bản\n",
    "class IndexedText(object):\n",
    "    # hàm khởi tạo cho lớp \n",
    "    def __init__(self, stemmer, text):\n",
    "        self._text = text\n",
    "        self._stemmer = stemmer\n",
    "        self._index = nltk.Index((self._stem(word), i)\n",
    "                                 for (i, word) in enumerate(text))      # tạo chỉ mục lưu trữ cặp (stemmed_word, index)\n",
    "\n",
    "    def concordance(self, word, width=40):\n",
    "        key = self._stem(word)          \n",
    "        wc = int(width/4)                # words of context\n",
    "        for i in self._index[key]:\n",
    "            lcontext = ' '.join(self._text[i-wc:i])\n",
    "            rcontext = ' '.join(self._text[i:i+wc])\n",
    "            ldisplay = '{:>{width}}'.format(lcontext[-width:], width=width)\n",
    "            rdisplay = '{:{width}}'.format(rcontext[:width], width=width)\n",
    "            print('index:',i)\n",
    "            print(ldisplay, rdisplay)\n",
    "            print()\n",
    "\n",
    "    def _stem(self, word):\n",
    "        return self._stemmer.stem(word).lower()\n",
    "    \n",
    "# khởi tạo stemmer là Porter\n",
    "porter = nltk.PorterStemmer()\n",
    "# khởi tạo text\n",
    "grail = nltk.corpus.webtext.words('grail.txt')\n",
    "\n",
    "# tạo class\n",
    "text = IndexedText(porter, grail)\n",
    "\n",
    "# gọi hàm concordance\n",
    "text.concordance('lie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>b. Lemmatization</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trình lemmatizer WordNet chỉ xóa các tiền tố nếu từ kết quả nằm trong từ điển của nó. Quá trình kiểm tra bổ sung này khiến trình lemmatizer chậm hơn các trình từ gốc ở trên. Lưu ý rằng nó không xử lý được việc nói dối, nhưng nó chuyển đổi phụ nữ thành phụ nữ.\n",
    "\n",
    "- Trình lemmatizer WordNet là một lựa chọn tốt nếu bạn muốn biên soạn từ vựng của một số văn bản và muốn có danh sách các lemma hợp lệ (hoặc từ khóa chính của từ điển)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond', 'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "\t\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "print([wnl.lemmatize(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>7. Regular Expression for Tokenization</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "pattern = r'''(?x)     # bật cờ để cho phép sử dụng biểu thức chính quy nhiều dòng\n",
    "    (?:[A-Z]\\.)+       # các chữ viết tắt, ví dụ như U.S.A.\n",
    "  | \\w+(?:-\\w+)*       # các từ có thể có dấu gạch nối bên trong\n",
    "  | \\$?\\d+(?:\\.\\d+)?%? # số tiền và tỷ lệ phần trăm, ví dụ như $12.40, 82%\n",
    "  | \\.\\.\\.             # dấu ba chấm\n",
    "  | [][.,;\"'?():-_`]   # các ký tự dấu câu riêng lẻ; bao gồm cả ], [\n",
    "'''\n",
    "\n",
    "# sử dụng regexp_tokenize() để thực hiện phân tách văn bản với nhiều regexp mà dễ đọc hơn\n",
    "nltk.regexp_tokenize(text, pattern)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
