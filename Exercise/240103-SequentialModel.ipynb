{"cells":[{"cell_type":"markdown","metadata":{},"source":["<h1 style = 'text-align: center'> <b>Week 03: Text Feature Extraction</b> </h1>"]},{"cell_type":"markdown","metadata":{},"source":["- Mentee: Võ Nguyễn Hoàng Kim\n","- Mentee ID: 240103"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T08:17:39.977754Z","iopub.status.busy":"2024-10-26T08:17:39.977256Z","iopub.status.idle":"2024-10-26T08:17:54.460746Z","shell.execute_reply":"2024-10-26T08:17:54.459174Z","shell.execute_reply.started":"2024-10-26T08:17:39.977688Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: underthesea in /opt/conda/lib/python3.10/site-packages (6.8.4)\n","Requirement already satisfied: Click>=6.0 in /opt/conda/lib/python3.10/site-packages (from underthesea) (8.1.7)\n","Requirement already satisfied: python-crfsuite>=0.9.6 in /opt/conda/lib/python3.10/site-packages (from underthesea) (0.9.11)\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from underthesea) (3.2.4)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from underthesea) (4.66.4)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from underthesea) (2.32.3)\n","Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.4.2)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.2.2)\n","Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from underthesea) (6.0.2)\n","Requirement already satisfied: underthesea-core==1.0.4 in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.0.4)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->underthesea) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (2024.8.30)\n","Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.26.4)\n","Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.14.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (3.5.0)\n"]}],"source":["!pip install underthesea\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","import underthesea\n","from underthesea import text_normalize, word_tokenize\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split, cross_val_score\n","import matplotlib.pyplot as pls\n","import seaborn as sns\n","from collections import Counter\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import re\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T08:17:54.464398Z","iopub.status.busy":"2024-10-26T08:17:54.463940Z","iopub.status.idle":"2024-10-26T08:17:54.854117Z","shell.execute_reply":"2024-10-26T08:17:54.852802Z","shell.execute_reply.started":"2024-10-26T08:17:54.464352Z"},"trusted":true},"outputs":[],"source":["# path of files\n","original_folder_path = '/kaggle/input/10000-vietnamese-books/output'\n","stop_words_path = '/kaggle/input/vietnamese-stop-words/vietnamese-stopwords.txt'\n","\n","# read data\n","def get_path_of_texts(orginal_folder_path):\n","    list_of_texts = [os.path.join(original_folder_path, text_path) \n","                     for text_path in os.listdir(original_folder_path)]\n","    return list_of_texts\n","\n","def read_file_txt(path_file, split_line = False):\n","    with open(path_file, 'r', encoding = 'utf-8') as f:\n","        file = f.read().lower()\n","    if(split_line):\n","        return file.splitlines()\n","    return file\n","\n","list_of_texts = get_path_of_texts(original_folder_path)\n","stop_words = read_file_txt(stop_words_path, split_line = True)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T08:18:01.460410Z","iopub.status.busy":"2024-10-26T08:18:01.459923Z","iopub.status.idle":"2024-10-26T08:18:01.476100Z","shell.execute_reply":"2024-10-26T08:18:01.474378Z","shell.execute_reply.started":"2024-10-26T08:18:01.460365Z"},"trusted":true},"outputs":[],"source":["# preprocessdata & split texts into tokens\n","def remove_stop_words(text, stop_words):\n","    # using regex with re.sub to remove stop word (better than using String replace)\n","    stop_words_pattern = r'\\b(?:' + '|'.join(map(re.escape, stop_words)) + r')\\b'\n","    cleaned_text = re.sub(stop_words_pattern, ' ', text)\n","    # remove redundant spaces\n","    cleaned_text = ' '.join(cleaned_text.split())\n","    return cleaned_text\n","\n","def preprocess_text(text_path, stop_words, idx = None):\n","    lowcase_text = read_file_txt(text_path)\n","    # preprocess raw text\n","    removed_text = remove_stop_words(lowcase_text, stop_words)\n","    normalized_text = text_normalize(removed_text)\n","    tokens = word_tokenize(normalized_text)\n","    print(idx, '..',end = '')\n","\n","    return tokens\n","    \n","# build vocab\n","def build_vocabs(list_of_tokens):\n","    all_tokens = [token for text_token in list_of_tokens for token in text_token ]\n","    word_count = Counter(all_tokens)\n","    vocab = sorted(word_count, key = word_count.get, reverse = True)\n","    word_to_idx = {word : idx for idx, word in enumerate(vocab, 1)}\n","    word_to_idx['<PAD>'] = 0\n","    idx_to_word = {idx : word for word, idx in word_to_idx.items()} \n","    return word_to_idx, idx_to_word\n","\n","# enconded text to indx \n","def encoded_texts(list_of_tokens, word_to_idx):\n","    encoded_texts = [[word_to_idx[word] for word in text_token] \n","                     for text_token in list_of_tokens]\n","    return encoded_texts        \n","\n","# put data to device \n","def to_tensor(encoded_texts, device):\n","    return [torch.tensor(text).to(device) for text in encoded_texts]\n","\n","# create sample for LSTM\n","def create_sequences(data, seq_length, device):\n","    sequences = []\n","    for i in range(len(data) - seq_length):\n","        seq = data[i : i + seq_length]    # Tạo chuỗi con dài seq_length\n","        label = data[i + seq_length]      # Nhãn là từ tiếp theo sau chuỗi\n","        sequences.append((seq.to(device), label.to(device)))\n","    return sequences        "]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T08:18:04.890346Z","iopub.status.busy":"2024-10-26T08:18:04.889732Z","iopub.status.idle":"2024-10-26T08:18:04.920753Z","shell.execute_reply":"2024-10-26T08:18:04.919399Z","shell.execute_reply.started":"2024-10-26T08:18:04.890299Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["8333 1041 1041\n"]}],"source":["# Split data into tran and test set\n","def train_test_split(file_paths, val_size = 0.1, test_size = 0.2):\n","    # shuffle the data\n","    random.shuffle(file_paths)\n","    total_size = len(file_paths)\n","    val_size = int(total_size * val_size)\n","    test_size = int(total_size * test_size)\n","    \n","    train_paths = file_paths[: -(val_size + test_size)]\n","    val_paths = file_paths[-(val_size + test_size): -test_size]\n","    test_paths = file_paths[-test_size:]\n","\n","    return train_paths, val_paths, test_paths\n","\n","# create Dataset class\n","class TextDataset(Dataset):\n","    def __init__(self, file_paths, word_to_idx, seq_length, device):\n","        self.file_paths = file_paths\n","        self.word_to_idx = word_to_idx\n","        self.seq_length = seq_length\n","        self.device = device\n","\n","    def __len__(self):\n","        return len(self.file_paths)\n","    \n","    def __getitem__(self, idx):\n","        file_path = self.file_paths[idx]\n","        tokens = preprocess_text(file_path, stop_words)\n","        encoded_text = [self.word_to_idx.get(token, self.word_to_idx[\"<PAD>\"]) for token in tokens]\n","        tensor_data = torch.tensor(encoded_text).to(self.device)\n","        \n","        sequences = []\n","        for i in range(len(tensor_data) - self.seq_length):\n","            seq = tensor_data[i : i + self.seq_length]\n","            label = tensor_data[i + self.seq_length]\n","            seq, label = seq.to(self.device), label.to(self.device)\n","            sequences.append((seq, label))\n","        return sequences\n","    \n","# create data loader\n","def create_dataloader(train_paths, val_paths, test_paths, word_to_idx, seq_length, batch_size = 32, device = 'cpu'):\n","    train_dataset = TextDataset(train_paths, word_to_idx, seq_length=10, device=device)\n","    val_dataset = TextDataset(val_paths, word_to_idx, seq_length=10, device=device)\n","    test_dataset = TextDataset(test_paths, word_to_idx, seq_length=10, device=device)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, val_loader, test_loader\n","\n","train_paths, val_paths, test_paths = train_test_split(list_of_texts, val_size=0.1, test_size=0.1)\n","print(len(train_paths), len(val_paths), len(test_paths))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T08:18:08.865487Z","iopub.status.busy":"2024-10-26T08:18:08.865013Z"},"trusted":true},"outputs":[],"source":["train_tokens = [preprocess_text(path, stop_words, idx) for idx, path in enumerate(train_paths)]\n","word_to_idx, idx_to_word = build_vocabs(train_tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# create LSTM model\n","class myLSTMModel(torch.nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n","        super(myLSTMModel, self).__init__()\n","        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n","        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n","        \n","    def forward(self, x):\n","        x = self.embedding(x)\n","        lstm_out, _ = self.lstm(x.view(len(x), 1, -1))\n","        x = self.fc(lstm_out[-1])\n","        return x\n","\n","def train(model, train_loader, val_loader, criterion, optimizer, device, num_epochs = 10):\n","    for epoch in range(num_epochs):\n","        print(f'Epoch [{epoch+1}/{num_epochs}]', end = ' ') \n","        model.train()\n","        train_loss = 0.0\n","        for sequences, labels in train_loader:\n","            sequences, labels = sequences.to(device), labels.to(device)\n","            \n","            optimizer.zero_grad()\n","            \n","            outputs = model(sequences)\n","            loss = criterion(outputs, labels)\n","            \n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","        \n","        # evaluate on the valid set\n","        model.eval()\n","        val_loss = 0.0\n","        with torch.no_grad():\n","            for sequences, labels in val_loader:\n","                sequences, labels = sequences.to(device), labels.to(device)\n","                outputs = model(sequences)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","        \n","        print(f\"Train loss: {train_loss/len(train_loader)}, Validation Loss: {val_loss/len(val_loader)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","\n","vocab_size = len(word_to_idx)\n","embedding_dim = 128\n","hidden_dim = 256\n","output_dim = vocab_size  \n","\n","model = myLSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","\n","learning_rate = 0.001\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","\n","num_epochs = 10\n","\n","seq_length = 10\n","batch_size = 32\n","\n","train_loader, val_loader, test_loader = create_dataloader(train_paths,val_paths, test_paths, word_to_idx, seq_length = seq_length,  batch_size=batch_size, device = device)\n","\n","train(model = model, train_loader = train_loader, val_loader = val_loader, \n","      criterion = criterion, optimizer = optimizer,  device = device, num_epochs = 20)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# evaluate model with test set\n","def evaluate(model, test_loader):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():\n","        for seqs, labels in test_loader:\n","            seqs, labels = seqs.to(device), labels.to(device)\n","            outputs = model(seqs)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","    \n","    avg_test_loss = total_loss / len(test_loader)\n","    print(f\"Test Loss: {avg_test_loss:.4f}\")\n","\n","# Gọi hàm đánh giá\n","evaluate(model, test_loader)\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":3046745,"sourceId":5236465,"sourceType":"datasetVersion"},{"datasetId":5742446,"sourceId":9448093,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
